{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f701fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import *\n",
    "\n",
    "import joblib\n",
    "from imblearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f1fb3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in the data\n",
    "train_file = 'C:/Users/Avani/OneDrive/Documents/Institute_of_data/capstone_bioreactor_cell_growth_prediction/train_data.xlsx'\n",
    "test_file = 'C:/Users/Avani/OneDrive/Documents/Institute_of_data/capstone_bioreactor_cell_growth_prediction/test_data.xlsx'\n",
    "X_train = pd.read_excel(train_file)\n",
    "y_train = pd.read_excel(train_file, sheet_name=1)\n",
    "X_test = pd.read_excel(test_file)\n",
    "y_test = pd.read_excel(test_file, sheet_name=1)\n",
    "\n",
    "#turn response variable 'dd10 CM content' into a binary class and store in new column\n",
    "y_train['y'] = y_train['dd10 CM Content']<90\n",
    "y_train['y'] = y_train['y'].astype('int64')\n",
    "y_test['y'] = y_test['dd10 CM Content']<90\n",
    "y_test['y'] = y_test['y'].astype('int64')\n",
    "\n",
    "#define target column for classification\n",
    "yc_train = y_train.y\n",
    "yc_test = y_test.y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101c05b6",
   "metadata": {},
   "source": [
    "Previously, I created a random forest model using all features from the dataset (see '2_comparing_models.ipynb' for details). After hypertuning the parameters, this model gave accuracy, precision, and recall over 88%. This model has been saved as 'best_randomforest.joblib'.\n",
    "\n",
    "If researchers want to make a prediction using all features, they can use 'pipe1' pipeline. This pipeline performs pre-processing using StandardScaler and oversampling using SMOTE. Then, predictions are made using all features and the random forest classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91d84f7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe1 = Pipeline([\n",
    "       ('scaler', StandardScaler()), \n",
    "       ('oversample', SMOTE(random_state=5)),\n",
    "       ('random forest', joblib.load(\"./best_randomforest.joblib\")), \n",
    "                ])\n",
    "                                \n",
    "\n",
    "#fit the pipeline with training data\n",
    "pipe1.fit(X_train, yc_train)\n",
    "\n",
    "#predict target values\n",
    "pipe1.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a90d2c",
   "metadata": {},
   "source": [
    "If researchers want to make a prediction using only 36 features only, they can use 'pipe2' pipeline. This pipeline performs pre-processing using StandardScaler and oversampling using SMOTE. Then, 36 features are selected using the random forest model. Finally, AdaBoost Classifier is used to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23acd16a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe2 = Pipeline([\n",
    "       ('scaler', StandardScaler()), \n",
    "       ('oversample', SMOTE(random_state=5)),\n",
    "       ('selection', SelectFromModel(joblib.load(\"./best_randomforest.joblib\"))), \n",
    "       ('AdaBoost', AdaBoostClassifier(n_estimators=300, random_state=7))\n",
    "                ])\n",
    "                                \n",
    "\n",
    "#fit the pipeline with training data\n",
    "pipe2.fit(X_train, yc_train)\n",
    "\n",
    "#predict target values\n",
    "pipe2.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb477f5",
   "metadata": {},
   "source": [
    "The next step would be to deploy these pipelines to Heroku using Flask App. This should be used as a proof of concept to confirm whether the model yields good results. Then, it can be scaled up by deployment onto AWS for researchers to use. It will be important that researchers can use the data from their ongoing experiments quickly to make a prediction and decide whether the experiment should be discontinued. Therefore, machine learning engieers would be needed to build cloud architecture to ensure data latency is not an issue."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
